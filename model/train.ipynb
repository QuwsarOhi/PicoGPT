{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "import os \n",
    "import io\n",
    "import pickle\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if hasattr(__builtins__,'__IPYTHON__'):\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "from model import GPT, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    context_len: int = 256\n",
    "    vocab_size: int = 128 \n",
    "    n_layer: int = 8\n",
    "    n_head: int = 2\n",
    "    n_embd: int = 64\n",
    "    dropout: float = 0.05\n",
    "    bias: bool = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    batch_size: int = 8\n",
    "    dtype: str = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "    device: str = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "    warmup_iters = 2000\n",
    "    learning_rate = 6e-4\n",
    "    lr_decay_iters = 600000\n",
    "    min_lr = 6e-5    \n",
    "    weight_decay = 1e-1\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# print(tokenizer.vocab_size)\n",
    "# print(tokenizer.encode(\"This is a sentence\"))\n",
    "# print(''.join(tokenizer.decode(tokenizer.encode(\"This is a sentence\"))))\n",
    "# print(''.join(tokenizer.decode([0] + tokenizer.encode(\"This is a sentence\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TINYSHAKESPERE\n",
    "# datapath = os.path.join(\n",
    "#     '..', \n",
    "#     'dataset',\n",
    "#     'tinyshakespeare.txt'\n",
    "# )\n",
    "\n",
    "# with open(datapath, 'r', encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "# n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "# train_data = data[:n]\n",
    "# val_data = data[n:]\n",
    "\n",
    "# def get_batch(split):\n",
    "#     # generate a small batch of data of inputs x and targets y\n",
    "#     data = train_data if split == 'train' else val_data\n",
    "#     ix = torch.randint(len(data) - GPTConfig.context_len, (TrainConfig.batch_size,))\n",
    "#     x = torch.stack([data[i:i+GPTConfig.context_len] for i in ix])\n",
    "#     y = torch.stack([data[i+1:i+GPTConfig.context_len+1] for i in ix])\n",
    "#     x, y = x.to(TrainConfig.device), y.to(TrainConfig.device)\n",
    "#     return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wiki_data = load_dataset(\"wikipedia\", \"20220301.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    N = len(wiki_data['train'])\n",
    "    \n",
    "    while True:\n",
    "        data = wiki_data['train'][torch.randint(N, (1,))]['text'][0]\n",
    "        if len(data) - GPTConfig.context_len > 0:\n",
    "            break\n",
    "        \n",
    "    ix = torch.randint(len(data) - GPTConfig.context_len, (TrainConfig.batch_size,))\n",
    "    x = torch.tensor([tokenizer.encode(data[i:i+GPTConfig.context_len]) for i in ix], \n",
    "                     dtype=torch.long)\n",
    "    y = torch.tensor([tokenizer.encode(data[i+1:i+GPTConfig.context_len+1]) for i in ix], \n",
    "                     dtype=torch.long)\n",
    "    x, y = x.to(TrainConfig.device), y.to(TrainConfig.device)\n",
    "    return x, y\n",
    "\n",
    "#print(get_batch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(GPTConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = get_batch('train')\n",
    "# print(x.shape, y.shape)\n",
    "# out, loss = model(x, y)\n",
    "# print(out.shape, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < TrainConfig.warmup_iters:\n",
    "        return TrainConfig.learning_rate * it / TrainConfig.warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > TrainConfig.lr_decay_iters:\n",
    "        return TrainConfig.min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - TrainConfig.warmup_iters) / (TrainConfig.lr_decay_iters - TrainConfig.warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return TrainConfig.min_lr + coeff * (TrainConfig.learning_rate - TrainConfig.min_lr)\n",
    "\n",
    "# x = list(range(5000000))\n",
    "# y = [get_lr(xx) for xx in x]\n",
    "# plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = model.configure_optimizers(TrainConfig.weight_decay, \n",
    "                                       TrainConfig.learning_rate, \n",
    "                                       (TrainConfig.beta1, TrainConfig.beta2), \n",
    "                                       TrainConfig.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ohi/Downloads/TinyLM/model/train.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://192.168.1.70:8080/home/ohi/Downloads/TinyLM/model/train.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# https://github.com/pytorch/pytorch/issues/16797#issuecomment-633423219\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://192.168.1.70:8080/home/ohi/Downloads/TinyLM/model/train.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCPU_Unpickler\u001b[39;00m(pickle\u001b[39m.\u001b[39mUnpickler):\n\u001b[1;32m      <a href='vscode-notebook-cell://192.168.1.70:8080/home/ohi/Downloads/TinyLM/model/train.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfind_class\u001b[39m(\u001b[39mself\u001b[39m, module, name):\n\u001b[1;32m      <a href='vscode-notebook-cell://192.168.1.70:8080/home/ohi/Downloads/TinyLM/model/train.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39mif\u001b[39;00m module \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtorch.storage\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_load_from_bytes\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# https://github.com/pytorch/pytorch/issues/16797#issuecomment-633423219\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else:\n",
    "            return super().find_class(module, name)\n",
    "\n",
    "\n",
    "def train_fn(model: nn.Module,\n",
    "             epoch: int,\n",
    "             optimizer: torch.optim.Optimizer,\n",
    "             savepath: str = None,\n",
    "             device='cpu'):\n",
    "\n",
    "    best_weight = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    iter_num = 0\n",
    "    train_phases = ['train', 'val']\n",
    "    losses = {phase: [] for phase in train_phases}\n",
    "\n",
    "    if savepath and os.path.exists(savepath):\n",
    "        # Try loading the model and weight\n",
    "        try:\n",
    "            with open(savepath, 'rb') as filehandler:\n",
    "                prev_train = CPU_Unpickler(filehandler).load()\n",
    "                best_weight = prev_train['best_weight']\n",
    "                model.load_state_dict(best_weight, strict=False)\n",
    "                losses = prev_train['losses']\n",
    "                best_loss = prev_train['best_loss']\n",
    "                optimizer = prev_train['optimizer'] \n",
    "                iter_num = prev_train['iter_num']\n",
    "                print(f\"Loaded model with loss: {best_loss:0.4f}\")\n",
    "        except:\n",
    "            print(f\"Could not load from path: {savepath}\")\n",
    "\n",
    "\n",
    "    for e in range(epoch):\n",
    "        for phase in train_phases:\n",
    "            is_training = (phase == 'train')\n",
    "            model.train() if is_training else model.eval()\n",
    "            loss, dats = 0., 0.,\n",
    "            tqdm_prog = tqdm(range(500))\n",
    "\n",
    "            for _ in tqdm_prog:\n",
    "                x, y = get_batch()\n",
    "                #x, y = x.to(device), y.to(device)\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    _, batch_loss = model(x, y)\n",
    "                    \n",
    "                    if is_training:\n",
    "                        batch_loss.backward()\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad(set_to_none=True)\n",
    "                        # LR scheduler\n",
    "                        lr = get_lr(iter_num)\n",
    "                        iter_num += 1\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "\n",
    "                # Stats\n",
    "                dats += x.size(0)\n",
    "                loss += batch_loss.item() * x.size(0)\n",
    "                tqdm_prog.set_description(f\"Epoch {e+1} [{phase.upper()}]: Loss: {loss/dats:.4f}\")\n",
    "\n",
    "            epoch_loss = loss / dats\n",
    "            losses[phase].append(epoch_loss)\n",
    "\n",
    "            if phase == \"val\" and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                print(f\"Best loss found: {best_loss:3.4f}\")\n",
    "            \n",
    "            best_weight = copy.deepcopy(model.state_dict())\n",
    "            if savepath:\n",
    "                with open(savepath, 'wb') as filehandler:\n",
    "                    pickle.dump({\n",
    "                        'best_weight': best_weight,\n",
    "                        'best_loss': best_loss,\n",
    "                        'losses': losses,\n",
    "                        'optimizer': optimizer,\n",
    "                        'iter_num': iter_num\n",
    "                    }, filehandler)\n",
    "        \n",
    "        x = torch.tensor(tokenizer.encode(\"he is a\"), dtype=torch.int).unsqueeze(0)\n",
    "        model.eval()\n",
    "        print(\"Inference:\", ''.join(tokenizer.decode(model.generate(x, max_new_tokens=500, \n",
    "                                                                    top_k=2).detach()[0].tolist())))\n",
    "\n",
    "        eps = list(range(1, len(losses['train'])+1))\n",
    "        fig = plt.figure()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.plot(eps, losses['train'], label='train', linestyle='dashed', \n",
    "                 color='tab:red')\n",
    "        plt.plot(eps, losses['val'], label='val', color='tab:red')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        plt.grid()\n",
    "        plt.savefig(os.path.join(\n",
    "            os.path.dirname(savepath),\n",
    "            'log.jpg'\n",
    "        ))\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(f'Best loss: {best_loss:3.4f}')\n",
    "    model.load_state_dict(best_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn(model, \n",
    "         200000,\n",
    "         optimizer,\n",
    "         os.path.join('..', 'logs', 'log.pkl'),\n",
    "         'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = os.path.join(\n",
    "    \"..\", \"logs\", \"log.pkl\"\n",
    ")\n",
    "\n",
    "def plot_graph(savepath):\n",
    "    with open(savepath, 'rb') as filehandler:\n",
    "        prev_train = CPU_Unpickler(filehandler).load()\n",
    "        best_weight = prev_train['best_weight']\n",
    "        model.load_state_dict(prev_train['best_weight'])\n",
    "        losses = prev_train['losses']\n",
    "        best_loss = prev_train['best_loss']\n",
    "        print(f\"Loaded model with loss: {best_loss:0.4f}\")\n",
    "\n",
    "    epochs = range(1, len(losses['train'])+1)\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss', color=color)\n",
    "    ax1.plot(epochs, losses['train'], color=color, linestyle='dashed', label='train')\n",
    "    ax1.plot(epochs, losses['val'], color=color, label='val')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    \n",
    "    plt.legend(loc=\"upper left\")\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_graph(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(tokenizer.encode(\"he is a\"), dtype=torch.int).unsqueeze(0)\n",
    "model.eval()\n",
    "# model.generate(self, idx, max_new_tokens, temperature=1.0, top_k=None)\n",
    "# %timeit ''.join(tokenizer.decode(model.generate(x, max_new_tokens=512, top_k=2).detach()[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "''.join(tokenizer.decode(model.generate(x, max_new_tokens=500, top_k=2).detach()[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
