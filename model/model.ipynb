{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "import inspect\n",
    "import os \n",
    "import io\n",
    "import pickle\n",
    "import torchtext\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if hasattr(__builtins__,'__IPYTHON__'):\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    context_len: int = 256\n",
    "    vocab_size: int = 128 \n",
    "    n_layer: int = 4\n",
    "    n_head: int = 2\n",
    "    n_embd: int = 64\n",
    "    dropout: float = 0.05\n",
    "    bias: bool = False \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    batch_size: int = 32\n",
    "    dtype: str = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "    device: str = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "    warmup_iters = 2000\n",
    "    learning_rate = 6e-4\n",
    "    lr_decay_iters = 600000\n",
    "    min_lr = 6e-5    \n",
    "    weight_decay = 1e-1\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Tokenizer():\n",
    "    '''\n",
    "    Very simple tokenizer that convers integer to ASCII\n",
    "    '''\n",
    "    __slots__ = ['vocab_size', 'enc', 'dec', 'specials']\n",
    "    \n",
    "    def __init__(self, specials: List[str] = ['<?>']) -> None:\n",
    "        self.enc = dict((c, i) for i, c in enumerate(specials))\n",
    "        self.dec = dict((i, c) for i, c in enumerate(specials))\n",
    "        self.specials = specials\n",
    "        self.__build()\n",
    "        \n",
    "    def __build(self):\n",
    "        n = len(self.specials)\n",
    "        for i, c in enumerate(string.printable):\n",
    "            self.enc[c] = i + n\n",
    "            self.dec[i + n] = c\n",
    "        # vocab size is 101\n",
    "        self.vocab_size = len(self.enc)\n",
    "            \n",
    "    def encode(self, x: List[str]) -> List[int]:\n",
    "        return [(self.enc[c] if c in self.enc else 0) for c in x]\n",
    "    \n",
    "    def decode(self, x: List[int]) -> List[str]:\n",
    "        return [(self.dec[i] if i in self.dec else self.dec[0]) for i in x]\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "# print(tokenizer.vocab_size)\n",
    "# print(tokenizer.encode(\"This is a sentence\"))\n",
    "# print(''.join(tokenizer.decode(tokenizer.encode(\"This is a sentence\"))))\n",
    "# print(''.join(tokenizer.decode([0] + tokenizer.encode(\"This is a sentence\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # linear is usually multiplied by 4\n",
    "        # here we have 2 for efficiency\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 2 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(2 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "# mlp = MLP(GPTConfig)\n",
    "# print(mlp(torch.rand(2, 2, 64)).shape)\n",
    "# del mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        # y.shape (B, nh, T, hs)\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, \n",
    "                                                             dropout_p=self.dropout if self.training else 0, \n",
    "                                                             is_causal=True)\n",
    "        \n",
    "        # re-assemble all head outputs side by side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) \n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "    \n",
    "# catt = CausalSelfAttention(GPTConfig)\n",
    "# print(catt(torch.rand(2, 2, 64)).shape)\n",
    "# del catt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "# block = Block(GPTConfig)\n",
    "# print(block(torch.rand(2, 2, 64)).shape)\n",
    "# del block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerger(nn.Module):\n",
    "    # https://github.com/lucidrains/vit-pytorch/blob/5578ac472faf3903d4739ba783f3875b77177e57/vit_pytorch/vit_with_patch_merger.py#L4\n",
    "\n",
    "    def __init__(self, dim, num_tokens_out):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** -0.5\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.queries = nn.Parameter(torch.randn(num_tokens_out, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        # queries.shape: [BS, M, DIM]\n",
    "        # x.shape: [BS, T, DIM]\n",
    "        # out: [M, DIM] x [DIM, T] = [M, T]\n",
    "        sim = torch.matmul(self.queries, x.transpose(-1, -2)) * self.scale\n",
    "        # att.shape: [BS, M, T]\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        # [M, T] x [T, DIM]\n",
    "        return torch.matmul(attn, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.context_len, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        \n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # https://paperswithcode.com/method/weight-tying\n",
    "        self.transformer.wte.weight = self.lm_head.weight \n",
    "        # self.patchmerger = PatchMerger(config.n_embd, 16)\n",
    "        # self.patchextend = PatchMerger(config.n_embd, 32)\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/((2 * config.n_layer)**0.5))\n",
    "        \n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.context_len, f\"Cannot forward sequence of length {t}, block size is only {self.config.context_len}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        # token embeddings of shape (b, t, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) \n",
    "        # position embeddings of shape (t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) \n",
    "        # add position embeddings\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        # propagating through transformers\n",
    "        for i, block in enumerate(self.transformer.h):\n",
    "            x = block(x)\n",
    "            # if i == 0: \n",
    "            #     x = self.patchmerger(x)\n",
    "            # elif i == self.config.n_layer-2:\n",
    "            #     x = self.patchextend(x)\n",
    "                \n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def crop_block_size(self, context_len):\n",
    "        # model surgery to decrease the block size if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        # but want to use a smaller block size for some smaller, simpler model\n",
    "        assert context_len <= self.config.context_len\n",
    "        self.config.context_len = context_len\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:context_len])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:,:,:context_len,:context_len]\n",
    "\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at context_len\n",
    "            idx_cond = idx if idx.size(1) <= self.config.context_len else idx[:, -self.config.context_len:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "    \n",
    "\n",
    "# gpt = GPT(GPTConfig)\n",
    "# #gpt = torch.compile(gpt)\n",
    "# out, loss = gpt(torch.ones((2, 8), dtype=torch.long))\n",
    "# print(out.shape, loss)\n",
    "# del gpt, out, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TINYSHAKESPERE\n",
    "\n",
    "# datapath = os.path.join(\n",
    "#     '..', \n",
    "#     'dataset',\n",
    "#     'tinyshakespeare.txt'\n",
    "# )\n",
    "\n",
    "# with open(datapath, 'r', encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "# n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "# train_data = data[:n]\n",
    "# val_data = data[n:]\n",
    "\n",
    "# def get_batch(split):\n",
    "#     # generate a small batch of data of inputs x and targets y\n",
    "#     data = train_data if split == 'train' else val_data\n",
    "#     ix = torch.randint(len(data) - GPTConfig.context_len, (TrainConfig.batch_size,))\n",
    "#     x = torch.stack([data[i:i+GPTConfig.context_len] for i in ix])\n",
    "#     y = torch.stack([data[i+1:i+GPTConfig.context_len+1] for i in ix])\n",
    "#     x, y = x.to(TrainConfig.device), y.to(TrainConfig.device)\n",
    "#     return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ohi/miniconda3/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wiki_data = load_dataset(\"wikipedia\", \"20220301.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    N = len(wiki_data['train'])\n",
    "    \n",
    "    while True:\n",
    "        data = wiki_data['train'][torch.randint(N, (1,))]['text'][0]\n",
    "        if len(data) - GPTConfig.context_len > 0:\n",
    "            break\n",
    "        \n",
    "    ix = torch.randint(len(data) - GPTConfig.context_len, (TrainConfig.batch_size,))\n",
    "    x = torch.tensor([tokenizer.encode(data[i:i+GPTConfig.context_len]) for i in ix], \n",
    "                     dtype=torch.long)\n",
    "    y = torch.tensor([tokenizer.encode(data[i+1:i+GPTConfig.context_len+1]) for i in ix], \n",
    "                     dtype=torch.long)\n",
    "    x, y = x.to(TrainConfig.device), y.to(TrainConfig.device)\n",
    "    return x, y\n",
    "\n",
    "#print(get_batch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.14M\n"
     ]
    }
   ],
   "source": [
    "model = GPT(GPTConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = get_batch('train')\n",
    "# print(x.shape, y.shape)\n",
    "# out, loss = model(x, y)\n",
    "# print(out.shape, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < TrainConfig.warmup_iters:\n",
    "        return TrainConfig.learning_rate * it / TrainConfig.warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > TrainConfig.lr_decay_iters:\n",
    "        return TrainConfig.min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - TrainConfig.warmup_iters) / (TrainConfig.lr_decay_iters - TrainConfig.warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return TrainConfig.min_lr + coeff * (TrainConfig.learning_rate - TrainConfig.min_lr)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# x = list(range(100000))\n",
    "# y = [get_lr(xx) for xx in x]\n",
    "# plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 18, with 153,920 parameters\n",
      "num non-decayed parameter tensors: 9, with 576 parameters\n",
      "using fused AdamW: False\n"
     ]
    }
   ],
   "source": [
    "optimizer = model.configure_optimizers(TrainConfig.weight_decay, \n",
    "                                       TrainConfig.learning_rate, \n",
    "                                       (TrainConfig.beta1, TrainConfig.beta2), \n",
    "                                       TrainConfig.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/issues/16797#issuecomment-633423219\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else:\n",
    "            return super().find_class(module, name)\n",
    "\n",
    "\n",
    "def train_fn(model: nn.Module,\n",
    "             epoch: int,\n",
    "             optimizer: torch.optim.Optimizer,\n",
    "             savepath: str = None,\n",
    "             device='cpu'):\n",
    "\n",
    "    best_weight = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    iter_num = 0\n",
    "    train_phases = ['train', 'val']\n",
    "    losses = {phase: [] for phase in train_phases}\n",
    "\n",
    "    if savepath and os.path.exists(savepath):\n",
    "        # Try loading the model and weight\n",
    "        try:\n",
    "            with open(savepath, 'rb') as filehandler:\n",
    "                prev_train = CPU_Unpickler(filehandler).load()\n",
    "                best_weight = prev_train['best_weight']\n",
    "                model.load_state_dict(best_weight, strict=False)\n",
    "                losses = prev_train['losses']\n",
    "                best_loss = prev_train['best_loss']\n",
    "                optimizer = prev_train['optimizer'] \n",
    "                iter_num = prev_train['iter_num']\n",
    "                print(f\"Loaded model with loss: {best_loss:0.4f}\")\n",
    "        except:\n",
    "            print(f\"Could not load from path: {savepath}\")\n",
    "\n",
    "\n",
    "    for e in range(epoch):\n",
    "        for phase in train_phases:\n",
    "            is_training = (phase == 'train')\n",
    "            model.train() if is_training else model.eval()\n",
    "            loss, dats = 0., 0.,\n",
    "            tqdm_prog = tqdm(range(1000))\n",
    "\n",
    "            for _ in tqdm_prog:\n",
    "                x, y = get_batch()\n",
    "                #x, y = x.to(device), y.to(device)\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    _, batch_loss = model(x, y)\n",
    "                    \n",
    "                    if is_training:\n",
    "                        batch_loss.backward()\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad(set_to_none=True)\n",
    "                        \n",
    "                        # LR scheduler\n",
    "                        lr = get_lr(iter_num)\n",
    "                        iter_num += 1\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "\n",
    "                # Stats\n",
    "                dats += x.size(0)\n",
    "                loss += batch_loss.item() * x.size(0)\n",
    "                tqdm_prog.set_description(f\"Epoch {e+1} [{phase.upper()}]: Loss: {loss/dats:.4f}\")\n",
    "                \n",
    "\n",
    "            epoch_loss = loss / dats\n",
    "            losses[phase].append(epoch_loss)\n",
    "\n",
    "            if phase == \"val\" and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_weight = copy.deepcopy(model.state_dict())\n",
    "                if savepath:\n",
    "                    with open(savepath, 'wb') as filehandler:\n",
    "                        pickle.dump({\n",
    "                            'best_weight': best_weight,\n",
    "                            'best_loss': best_loss,\n",
    "                            'losses': losses,\n",
    "                            'optimizer': optimizer,\n",
    "                            'iter_num': iter_num\n",
    "                        }, filehandler)\n",
    "                print(f\"Best loss found: {best_loss:3.4f}\")\n",
    "            \n",
    "        eps = list(range(1, len(losses['train'])+1))\n",
    "        fig = plt.figure()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.plot(eps, losses['train'], label='train', linestyle='dashed', \n",
    "                 color='tab:red')\n",
    "        plt.plot(eps, losses['val'], label='val', color='tab:red')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        plt.grid()\n",
    "        plt.savefig(os.path.join(\n",
    "            os.path.dirname(savepath),\n",
    "            'log.jpg'\n",
    "        ))\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(f'Best loss: {best_loss:3.4f}')\n",
    "    model.load_state_dict(best_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with loss: 2.4728\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b416d649739040169636f73b8fd02781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7855d96b519240e8b7e2ff8bf677e572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss: 2.4728\n"
     ]
    }
   ],
   "source": [
    "train_fn(model, \n",
    "         200000,\n",
    "         optimizer,\n",
    "         os.path.join('..', 'logs', 'log.pkl'),\n",
    "         'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with loss: 2.4728\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "savepath = os.path.join(\n",
    "    \"..\", \"logs\", \"log.pkl\"\n",
    ")\n",
    "\n",
    "def plot_graph(savepath):\n",
    "    with open(savepath, 'rb') as filehandler:\n",
    "        prev_train = CPU_Unpickler(filehandler).load()\n",
    "        # best_weight = prev_train['best_weight']\n",
    "        # model.load_state_dict(best_weight)\n",
    "        losses = prev_train['losses']\n",
    "        best_loss = prev_train['best_loss']\n",
    "        print(f\"Loaded model with loss: {best_loss:0.4f}\")\n",
    "\n",
    "    epochs = range(1, len(losses['train'])+1)\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss', color=color)\n",
    "    ax1.plot(epochs, losses['train'], color=color, linestyle='dashed', label='train')\n",
    "    ax1.plot(epochs, losses['val'], color=color, label='val')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    \n",
    "    plt.legend(loc=\"upper left\")\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#plot_graph(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is aof ico thtth elaloracee?\\nTirk.\\nI\\nERIREVTEKELK: al syin-\\nW wo f p smely, acou ined akshhishar I d hea'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.tensor(tokenizer.encode(\"he is a\"), dtype=torch.int).unsqueeze(0)\n",
    "# model.eval()\n",
    "# # model.generate(self, idx, max_new_tokens, temperature=1.0, top_k=None)\n",
    "# ''.join(tokenizer.decode(model.generate(x, max_new_tokens=100).detach()[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
